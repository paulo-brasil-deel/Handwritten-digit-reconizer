{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):    \n",
    "    #sizes is an array and the number of nodes correspondes to the respective possition of the array\n",
    "    def __init__(self,sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "        \n",
    "            \n",
    "    #sigmoide activation function\n",
    "    def seigmoid(z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    \n",
    "    #maybe my activation function...it passes to the next node a value : a\n",
    "    #that is the result of the , so that the next node takes that and does another operation.\n",
    "    \n",
    "    def feedforward(self,a):\n",
    "        for b,w in zip(self.biases,self,weights):\n",
    "            a = sigmoid(np.dot(w,a) + b)\n",
    "        return a\n",
    "    \n",
    "       \n",
    "    \n",
    "    #training_data is a lista of tuples (x,y) that correspondes to the input and the right outcome\n",
    "    #epochs is the number of training times\n",
    "    # eta is the learning variable.\n",
    "    #if test_data is provided the network will print the parcial progress of the training.\n",
    "    \n",
    "    def SGD (self, training_data, epochs, mini_batch_size, eta, testa_data=None):\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            random.suffle(training_data)\n",
    "            mini_batchs = [training_data[k:k+mini_batch_size] for k in range (0,n,mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            if test_data:\n",
    "                return 0\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} finished\".format(j))\n",
    "                \n",
    "\n",
    "\n",
    "#     updates the weights and biases of the network playing gradiente descente using \n",
    "#     backpropagation for a mini batch. The mini_batch is a list of tuples(x,y) and eta\n",
    "#     is the learning rate\n",
    "    \n",
    "    def update_mini_batch(self,mini_batch,eta):\n",
    "        \n",
    "        #np.shape returna a new array with the size given filled with zeroes.\n",
    "        nabla_b = [np.zeroes(b.shape) for b in self.baises]\n",
    "        nabla_w = [np.zeroes(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x,y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backpropagation(x,y)\n",
    "            nabla_b = [nb+dmb for nb,dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(delta_w, delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.baises = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.baises, nabla_b)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def backpropagation(self,x,y):\n",
    "#         Returns a tuple '(nabla_b, nabla_w)' that represents the gradiente\n",
    "#         for the cost function C_x. nabla_b, nabla_w are list of the numpy\n",
    "#         matrix layer just like self.biases and self.wights\n",
    "        \n",
    "        \n",
    "        nabla_b = [np.zeroes(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeroes(w.shape) for w in selg.weights]\n",
    "        \n",
    "        #Feedforward\n",
    "        activation = x\n",
    "        \n",
    "        #list that gathers all the activations, layer by layer\n",
    "        activations = [x]\n",
    "        \n",
    "        #list for gathering all the vectors z, layer by layer\n",
    "        \n",
    "        zs = []\n",
    "        \n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #here we are doing a dot product of our weights and the activation function\n",
    "            #of eache node and adding to the biaises.\n",
    "            z = np.dot(w,activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            #in the end we will have an array of all the results of the activation functions\n",
    "            activations.append(activation)\n",
    "        \n",
    "        \n",
    "        #backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # here l = 1 means the last layer of the neurons and l = 2\n",
    "        # means the second and so forth.\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "        return (nabla_b, nabla_w)\n",
    "                \n",
    "                \n",
    "                \n",
    "            \n",
    "          \n",
    "               \n",
    "\n",
    "                      \n",
    "                      \n",
    "                \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sizes = \n",
      "[1, 2]\n",
      "baises = \n",
      "[array([[ 1.65538028],\n",
      "       [-0.78619631]])]\n",
      "wights = \n",
      "[array([[0.23103963],\n",
      "       [0.62875769]])]\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def neural (n):\n",
    "    sizes = n\n",
    "    wights = [np.random.randn(y,1) for y in n[1:]]\n",
    "    baises = [np.random.randn(y,x) for x,y in zip(n[:-1],n[1:])]\n",
    "    Wight = sizes\n",
    "    Baises = wights\n",
    "    Sizes = baises\n",
    "    \n",
    "    print('sizes = ')\n",
    "    print(sizes)\n",
    "    print('baises = ')\n",
    "    print(baises)\n",
    "    print('wights = ')\n",
    "    print(wights)\n",
    "    \n",
    "\n",
    "neural([1,2])\n",
    "print (Sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[([1, 2], [5, 6]), ([3, 4], [7, 8])]\n",
      "b =  [1, 2]\n",
      "w =  [5, 6]\n",
      "b =  [3, 4]\n",
      "w =  [7, 8]\n"
     ]
    }
   ],
   "source": [
    "def u(a,baises,weights):\n",
    "    g = list(zip(baises,weights))\n",
    "    print(g)\n",
    "    for b,w in zip(baises,weights):\n",
    "         print('b = ',b)\n",
    "         print('w = ',w)\n",
    "    \n",
    "   \n",
    "\n",
    "        \n",
    "    \n",
    "    \n",
    "u([2,3],[[1,2],[3,4]],[[5,6],[7,8]])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
