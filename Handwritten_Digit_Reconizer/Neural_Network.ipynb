{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):    \n",
    "    #sizes is an array and the number of nodes correspondes to the respective possition of the array\n",
    "    def __init__(self,sizes):\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.biases = [np.random.randn(y,1) for y in sizes[1:]]\n",
    "        self.weights = [np.random.randn(y,x) for x,y in zip(sizes[:-1],sizes[1:])]\n",
    "        \n",
    "            \n",
    "    #sigmoide activation function\n",
    "    def seigmoid(z):\n",
    "        return 1.0/(1.0 + np.exp(-z))\n",
    "    \n",
    "    def sigmoid_prime(z):\n",
    "    #Derivative of the sigmoid function\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "    \n",
    "    \n",
    "    #maybe my activation function...it passes to the next node a value : a\n",
    "    #that is the result of the , so that the next node takes that and does another operation.\n",
    "    \n",
    "    def feedforward(self,a):\n",
    "        for b,w in zip(self.biases,self,weights):\n",
    "            a = sigmoid(np.dot(w,a) + b)\n",
    "        return a\n",
    "    \n",
    "       \n",
    "    \n",
    "    #training_data is a lista of tuples (x,y) that correspondes to the input and the right outcome\n",
    "    #epochs is the number of training times\n",
    "    # eta is the learning variable.\n",
    "    #if test_data is provided the network will print the parcial progress of the training.\n",
    "    \n",
    "    def SGD (self, training_data, epochs, mini_batch_size, eta, testa_data=None):\n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        \n",
    "        if test_data:\n",
    "            test_data = list(test_data)\n",
    "            n_test = len(test_data)\n",
    "        \n",
    "        for j in range(epochs):\n",
    "            \n",
    "            random.suffle(training_data)\n",
    "            mini_batches = [training_data[k:k+mini_batch_size] for k in range (0,n,mini_batch_size)]\n",
    "            \n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(mini_batch, eta)\n",
    "            \n",
    "            if test_data:\n",
    "                return 0\n",
    "                print(\"Epoch {} : {} / {}\".format(j,self.evaluate(test_data), n_test))\n",
    "            else:\n",
    "                print(\"Epoch {} finished\".format(j))\n",
    "                \n",
    "\n",
    "\n",
    "#     updates the weights and biases of the network playing gradiente descente using \n",
    "#     backpropagation for a mini batch. The mini_batch is a list of tuples(x,y) and eta\n",
    "#     is the learning rate\n",
    "    \n",
    "    def update_mini_batch(self,mini_batch,eta):\n",
    "        \n",
    "        #np.shape returna a new array with the size given filled with zeroes.\n",
    "        nabla_b = [np.zeroes(b.shape) for b in self.baises]\n",
    "        nabla_w = [np.zeroes(w.shape) for w in self.weights]\n",
    "        \n",
    "        for x,y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backpropagation(x,y)\n",
    "            nabla_b = [nb+dnb for nb,dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(delta_w, delta_nabla_w)]\n",
    "        \n",
    "        self.weights = [w-(eta/len(mini_batch))*nw for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.baises = [b-(eta/len(mini_batch))*nb for b, nb in zip(self.baises, nabla_b)]\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def backpropagation(self,x,y):\n",
    "#         Returns a tuple '(nabla_b, nabla_w)' that represents the gradiente\n",
    "#         for the cost function C_x. nabla_b, nabla_w are list of the numpy\n",
    "#         matrix layer just like self.biases and self.wights\n",
    "        \n",
    "        \n",
    "        nabla_b = [np.zeroes(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeroes(w.shape) for w in selg.weights]\n",
    "        \n",
    "        #Feedforward\n",
    "        activation = x\n",
    "        \n",
    "        #list that gathers all the activations, layer by layer\n",
    "        activations = [x]\n",
    "        \n",
    "        #list for gathering all the z vectors, layer by layer        \n",
    "        zs = []\n",
    "        \n",
    "        \n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            #here we are doing a dot product of our weights and the activation function\n",
    "            #of each node and adding the biaises vector .\n",
    "            z = np.dot(w,activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            #in the end we will have an array of all the results of the activation functions\n",
    "            activations.append(activation)\n",
    "        \n",
    "        \n",
    "        #backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        \n",
    "        # here l = 1 means the last layer of the neurons and l = 2\n",
    "        # means the second to last and so forth.\n",
    "        \n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        \n",
    "        return (nabla_b, nabla_w)\n",
    "                \n",
    "    \n",
    "    \n",
    "    def evaluate(self, test_data):\n",
    "#         Return the number of test inputs for which the neural\n",
    "#         network outputs the correct result. Note that the neural\n",
    "#         network's output is assumed to be the index of whichever\n",
    "#         neuron in the final layer has the highest activation.\n",
    "        test_results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in test_data]\n",
    "        return sum(int(x == y) for (x, y) in test_results)\n",
    "\n",
    "    def cost_derivative(self, output_activations, y):\n",
    "      \n",
    "        return (output_activations-y)           \n",
    "                \n",
    "            \n",
    "          \n",
    "               \n",
    "\n",
    "                           "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
