{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import sys\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "#### Define the quadratic and cross-entropy cost functions\n",
    "\n",
    "class QuadraticCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "        \"\"\"Return the cost associated with an output ``a`` and desired output\n",
    "        ``y``.\n",
    "\n",
    "        \"\"\"\n",
    "        return 0.5*np.linalg.norm(a-y)**2\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "        \"\"\"Return the error delta from the output layer.\"\"\"\n",
    "        return (a-y) * sigmoid_prime(z)\n",
    "\n",
    "\n",
    "class CrossEntropyCost(object):\n",
    "\n",
    "    @staticmethod\n",
    "    def fn(a, y):\n",
    "#         Return the cost associated with an output ``a`` and desired output\n",
    "#         ``y``.  Note that np.nan_to_num is used to ensure numerical\n",
    "#         stability. \n",
    "        \n",
    "        return np.sum(np.nan_to_num(-y*np.log(a)-(1-y)*np.log(1-a)))\n",
    "\n",
    "    @staticmethod\n",
    "    def delta(z, a, y):\n",
    "#         Return the error delta from the output layer.\n",
    "        \n",
    "        return (a-y)\n",
    "\n",
    "\n",
    "#### Main Network class\n",
    "class Network(object):\n",
    "\n",
    "    def __init__(self, sizes, cost=CrossEntropyCost):\n",
    "#         The list ``sizes`` contains the number of neurons in the respective\n",
    "#         layers of the network.  For example, if the list was [2, 3, 1]\n",
    "#         then it would be a three-layer network, with the first layer\n",
    "#         containing 2 neurons, the second layer 3 neurons, and the\n",
    "#         third layer 1 neuron. \n",
    "\n",
    "        self.num_layers = len(sizes)\n",
    "        self.sizes = sizes\n",
    "        self.default_weight_initializer()\n",
    "        self.cost=cost\n",
    "\n",
    "    def default_weight_initializer(self):\n",
    "#         Initialize each weight using a Gaussian distribution with mean 0\n",
    "#         and standard deviation 1 over the square root of the number of\n",
    "#         weights connecting to the same neuron.  Initialize the biases\n",
    "#         using a Gaussian distribution with mean 0 and standard\n",
    "#         deviation 1.\n",
    "        \n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)/np.sqrt(x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def large_weight_initializer(self):\n",
    "        \n",
    "        self.biases = [np.random.randn(y, 1) for y in self.sizes[1:]]\n",
    "        self.weights = [np.random.randn(y, x)\n",
    "                        for x, y in zip(self.sizes[:-1], self.sizes[1:])]\n",
    "\n",
    "    def feedforward(self, a):\n",
    "#         Return the output of the network if ``a`` is input.\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "    def SGD(self, training_data, epochs, mini_batch_size, eta,\n",
    "            lmbda = 0.0, #regularization parameter\n",
    "            evaluation_data=None,\n",
    "            monitor_evaluation_cost=False,\n",
    "            monitor_evaluation_accuracy=False,\n",
    "            monitor_training_cost=False,\n",
    "            monitor_training_accuracy=False):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic gradient\n",
    "        descent.  The ``training_data`` is a list of tuples ``(x, y)``\n",
    "        representing the training inputs and the desired outputs.  The\n",
    "        other non-optional parameters are self-explanatory, as is the\n",
    "        regularization parameter ``lmbda``.  The method also accepts\n",
    "        ``evaluation_data``, usually either the validation or test\n",
    "        data.  We can monitor the cost and accuracy on either the\n",
    "        evaluation data or the training data, by setting the\n",
    "        appropriate flags.  The method returns a tuple containing four\n",
    "        lists: the (per-epoch) costs on the evaluation data, the\n",
    "        accuracies on the evaluation data, the costs on the training\n",
    "        data, and the accuracies on the training data.  All values are\n",
    "        evaluated at the end of each training epoch.  So, for example,\n",
    "        if we train for 30 epochs, then the first element of the tuple\n",
    "        will be a 30-element list containing the cost on the\n",
    "        evaluation data at the end of each epoch. Note that the lists\n",
    "        are empty if the corresponding flag is not set.\n",
    "\n",
    "        \"\"\"\n",
    "        if evaluation_data:\n",
    "                        \n",
    "            evaluation_data = list(evaluation_data)\n",
    "            n_data = len(evaluation_data)\n",
    "            \n",
    "        training_data = list(training_data)\n",
    "        n = len(training_data)\n",
    "        evaluation_cost, evaluation_accuracy = [], []\n",
    "        training_cost, training_accuracy = [], []\n",
    "        for j in range(epochs):\n",
    "            random.shuffle(training_data)\n",
    "            mini_batches = [\n",
    "                training_data[k:k+mini_batch_size]\n",
    "                for k in range(0, n, mini_batch_size)]\n",
    "            for mini_batch in mini_batches:\n",
    "                self.update_mini_batch(\n",
    "                    mini_batch, eta, lmbda, len(training_data))\n",
    "            print(\"Epoch {} finished\".format(j))\n",
    "           \n",
    "            if monitor_training_cost:\n",
    "                cost = self.total_cost(training_data, lmbda)\n",
    "                training_cost.append(cost)                 \n",
    "                print (\"Cost on training data: {}\".format(cost))\n",
    "            if monitor_training_accuracy:\n",
    "                accuracy = self.accuracy(training_data, convert=True)\n",
    "                training_accuracy.append(accuracy)\n",
    "                print (\"Accuracy on training data: {} / {}\".format(\n",
    "                    accuracy, n))\n",
    "            if monitor_evaluation_cost:\n",
    "                cost = self.total_cost(evaluation_data, lmbda, convert=True)\n",
    "                evaluation_cost.append(cost)\n",
    "                print (\"Cost on evaluation data: {}\".format(cost))\n",
    "            if monitor_evaluation_accuracy:\n",
    "                accuracy = self.accuracy(evaluation_data)\n",
    "                evaluation_accuracy.append(accuracy)\n",
    "                print (\"Accuracy on evaluation data: {} / {}\".format(\n",
    "                    self.accuracy(evaluation_data), n_data))\n",
    "            print\n",
    "        return evaluation_cost, evaluation_accuracy, \\\n",
    "            training_cost, training_accuracy\n",
    "\n",
    "    def update_mini_batch(self, mini_batch, eta, lmbda, n):\n",
    "#         Update the network's weights and biases by applying gradient\n",
    "#         descent using backpropagation to a single mini batch.\n",
    "#         The mini_batch is a list of tuples (x, y)\n",
    "#         eta = learning rate\n",
    "#         lmbda = regularization parameter        \n",
    "#         n = total size of the training data set.\n",
    "\n",
    "        \n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch:\n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y)\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        self.weights = [(1-eta*(lmbda/n))*w-(eta/len(mini_batch))*nw\n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb\n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "\n",
    "    def backprop(self, x, y):\n",
    "#         Returns a tuple (nabla_b, nabla_w) representing the gradient for the cost function C_x. \n",
    "#         nabla_b and nabla_w are layer-by-layer lists of numpy arrays\n",
    "\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = (self.cost).delta(zs[-1], activations[-1], y)\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in range(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "    def accuracy(self, data, convert=False):\n",
    "        \n",
    "        if convert:\n",
    "            results = [(np.argmax(self.feedforward(x)), np.argmax(y))\n",
    "                       for (x, y) in data]\n",
    "        else:\n",
    "            results = [(np.argmax(self.feedforward(x)), y)\n",
    "                        for (x, y) in data]\n",
    "        return sum(int(x == y) for (x, y) in results)\n",
    "\n",
    "    def total_cost(self, data, lmbda, convert=False):\n",
    "#         Return the total cost for the data set data.The flag\n",
    "#         convert should be set to False if the data set is the\n",
    "#         training data (the usual case), and to True if the data set is\n",
    "#         the validation or test data.  See comments on the similar (but\n",
    "#         reversed) convention for the accuracy method, above.\n",
    "        \n",
    "        cost = 0.0\n",
    "        for x, y in data:\n",
    "            a = self.feedforward(x)\n",
    "            if convert: y = vectorized_result(y)\n",
    "            cost += self.cost.fn(a, y)/len(data)\n",
    "        cost += 0.5*(lmbda/len(data))*sum(\n",
    "            np.linalg.norm(w)**2 for w in self.weights)\n",
    "        return cost\n",
    "\n",
    "    def save(self, filename):\n",
    "        #Save the neural network to the file ``filename``.\n",
    "        data = {\"sizes\": self.sizes,\n",
    "                \"weights\": [w.tolist() for w in self.weights],\n",
    "                \"biases\": [b.tolist() for b in self.biases],\n",
    "                \"cost\": str(self.cost.__name__)}\n",
    "        f = open(filename, \"w\")\n",
    "        json.dump(data, f)\n",
    "        f.close()\n",
    "\n",
    "\n",
    "def load(filename):\n",
    "#     Load a neural network from the file ``filename``.  Returns an\n",
    "#     instance of Network.\n",
    "\n",
    "    \n",
    "    f = open(filename, \"r\")\n",
    "    data = json.load(f)\n",
    "    f.close()\n",
    "    cost = getattr(sys.modules[__name__], data[\"cost\"])\n",
    "    net = Network(data[\"sizes\"], cost=cost)\n",
    "    net.weights = [np.array(w) for w in data[\"weights\"]]\n",
    "    net.biases = [np.array(b) for b in data[\"biases\"]]\n",
    "    return net\n",
    "\n",
    "\n",
    "def vectorized_result(j):\n",
    "#     Return a 10-dimensional unit vector with a 1.0 in the j'th position\n",
    "#     and zeroes elsewhere.  This is used to convert a digit (0...9)\n",
    "#     into a corresponding desired output from the neural network.\n",
    "\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def sigmoid(z):\n",
    "    #The sigmoid function.\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "def sigmoid_prime(z):\n",
    "    #Derivative of the sigmoid function\n",
    "    return sigmoid(z)*(1-sigmoid(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mnist_loader\n",
    "~~~~~~~~~~~~\n",
    "A library to load the MNIST image data.  For details of the data\n",
    "structures that are returned, see the doc strings for ``load_data``\n",
    "and ``load_data_wrapper``.  In practice, ``load_data_wrapper`` is the\n",
    "function usually called by our neural network code.\n",
    "\"\"\"\n",
    "\n",
    "#### Libraries\n",
    "# Standard library\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "\n",
    "# Third-party libraries\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Return the MNIST data as a tuple containing the training data,\n",
    "    the validation data, and the test data.\n",
    "    The ``training_data`` is returned as a tuple with two entries.\n",
    "    The first entry contains the actual training images.  This is a\n",
    "    numpy ndarray with 50,000 entries.  Each entry is, in turn, a\n",
    "    numpy ndarray with 784 values, representing the 28 * 28 = 784\n",
    "    pixels in a single MNIST image.\n",
    "    The second entry in the ``training_data`` tuple is a numpy ndarray\n",
    "    containing 50,000 entries.  Those entries are just the digit\n",
    "    values (0...9) for the corresponding images contained in the first\n",
    "    entry of the tuple.\n",
    "    The ``validation_data`` and ``test_data`` are similar, except\n",
    "    each contains only 10,000 images.\n",
    "    This is a nice data format, but for use in neural networks it's\n",
    "    helpful to modify the format of the ``training_data`` a little.\n",
    "    That's done in the wrapper function ``load_data_wrapper()``, see\n",
    "    below.\n",
    "    \"\"\"\n",
    "    f = gzip.open('./mnist.pkl.gz', 'rb')\n",
    "    training_data, validation_data, test_data = pickle.load(f, encoding='iso-8859-1')\n",
    "    f.close()\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def load_data_wrapper():\n",
    "    \"\"\"Return a tuple containing ``(training_data, validation_data,\n",
    "    test_data)``. Based on ``load_data``, but the format is more\n",
    "    convenient for use in our implementation of neural networks.\n",
    "    In particular, ``training_data`` is a list containing 50,000\n",
    "    2-tuples ``(x, y)``.  ``x`` is a 784-dimensional numpy.ndarray\n",
    "    containing the input image.  ``y`` is a 10-dimensional\n",
    "    numpy.ndarray representing the unit vector corresponding to the\n",
    "    correct digit for ``x``.\n",
    "    ``validation_data`` and ``test_data`` are lists containing 10,000\n",
    "    2-tuples ``(x, y)``.  In each case, ``x`` is a 784-dimensional\n",
    "    numpy.ndarry containing the input image, and ``y`` is the\n",
    "    corresponding classification, i.e., the digit values (integers)\n",
    "    corresponding to ``x``.\n",
    "    Obviously, this means we're using slightly different formats for\n",
    "    the training data and the validation / test data.  These formats\n",
    "    turn out to be the most convenient for use in our neural network\n",
    "    code.\"\"\"\n",
    "    tr_d, va_d, te_d = load_data()\n",
    "    training_inputs = [np.reshape(x, (784, 1)) for x in tr_d[0]]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    validation_inputs = [np.reshape(x, (784, 1)) for x in va_d[0]]\n",
    "    validation_data = zip(validation_inputs, va_d[1])\n",
    "    test_inputs = [np.reshape(x, (784, 1)) for x in te_d[0]]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    return (training_data, validation_data, test_data)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    \"\"\"Return a 10-dimensional unit vector with a 1.0 in the jth\n",
    "    position and zeroes elsewhere.  This is used to convert a digit\n",
    "    (0...9) into a corresponding desired output from the neural\n",
    "    network.\"\"\"\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data, validation_data, test_data = load_data_wrapper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10], cost=CrossEntropyCost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished\n",
      "Cost on training data: 0.49861334110702016\n",
      "Accuracy on training data: 46918 / 50000\n",
      "Cost on evaluation data: 0.8060758939928532\n",
      "Accuracy on evaluation data: 9404 / 10000\n",
      "Epoch 1 finished\n",
      "Cost on training data: 0.44704183452272317\n",
      "Accuracy on training data: 47613 / 50000\n",
      "Cost on evaluation data: 0.8541352821096009\n",
      "Accuracy on evaluation data: 9494 / 10000\n",
      "Epoch 2 finished\n",
      "Cost on training data: 0.4222718170221894\n",
      "Accuracy on training data: 47898 / 50000\n",
      "Cost on evaluation data: 0.8902788838316347\n",
      "Accuracy on evaluation data: 9527 / 10000\n",
      "Epoch 3 finished\n",
      "Cost on training data: 0.40518845628272937\n",
      "Accuracy on training data: 47967 / 50000\n",
      "Cost on evaluation data: 0.8958818274455034\n",
      "Accuracy on evaluation data: 9561 / 10000\n",
      "Epoch 4 finished\n",
      "Cost on training data: 0.3959622088613118\n",
      "Accuracy on training data: 48121 / 50000\n",
      "Cost on evaluation data: 0.9080913552600751\n",
      "Accuracy on evaluation data: 9567 / 10000\n",
      "Epoch 5 finished\n",
      "Cost on training data: 0.3857796520157049\n",
      "Accuracy on training data: 48151 / 50000\n",
      "Cost on evaluation data: 0.9146304304273882\n",
      "Accuracy on evaluation data: 9574 / 10000\n",
      "Epoch 6 finished\n",
      "Cost on training data: 0.39863137155241113\n",
      "Accuracy on training data: 48038 / 50000\n",
      "Cost on evaluation data: 0.9357288455719278\n",
      "Accuracy on evaluation data: 9567 / 10000\n",
      "Epoch 7 finished\n",
      "Cost on training data: 0.3860621541637011\n",
      "Accuracy on training data: 48287 / 50000\n",
      "Cost on evaluation data: 0.9216804361350406\n",
      "Accuracy on evaluation data: 9590 / 10000\n",
      "Epoch 8 finished\n",
      "Cost on training data: 0.3898062600109953\n",
      "Accuracy on training data: 48250 / 50000\n",
      "Cost on evaluation data: 0.9354999911181375\n",
      "Accuracy on evaluation data: 9578 / 10000\n",
      "Epoch 9 finished\n",
      "Cost on training data: 0.3961811046382846\n",
      "Accuracy on training data: 48077 / 50000\n",
      "Cost on evaluation data: 0.9544562972892887\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "Epoch 10 finished\n",
      "Cost on training data: 0.3929240474516481\n",
      "Accuracy on training data: 48255 / 50000\n",
      "Cost on evaluation data: 0.9434817734097319\n",
      "Accuracy on evaluation data: 9597 / 10000\n",
      "Epoch 11 finished\n",
      "Cost on training data: 0.3685762153672637\n",
      "Accuracy on training data: 48408 / 50000\n",
      "Cost on evaluation data: 0.9273468769595854\n",
      "Accuracy on evaluation data: 9620 / 10000\n",
      "Epoch 12 finished\n",
      "Cost on training data: 0.37326766728163746\n",
      "Accuracy on training data: 48355 / 50000\n",
      "Cost on evaluation data: 0.9337514447400954\n",
      "Accuracy on evaluation data: 9609 / 10000\n",
      "Epoch 13 finished\n",
      "Cost on training data: 0.41901052658864035\n",
      "Accuracy on training data: 47935 / 50000\n",
      "Cost on evaluation data: 0.9932563665210454\n",
      "Accuracy on evaluation data: 9506 / 10000\n",
      "Epoch 14 finished\n",
      "Cost on training data: 0.3973309381719917\n",
      "Accuracy on training data: 48255 / 50000\n",
      "Cost on evaluation data: 0.9673846439002207\n",
      "Accuracy on evaluation data: 9564 / 10000\n",
      "Epoch 15 finished\n",
      "Cost on training data: 0.3746451906630837\n",
      "Accuracy on training data: 48403 / 50000\n",
      "Cost on evaluation data: 0.9398237025245462\n",
      "Accuracy on evaluation data: 9604 / 10000\n",
      "Epoch 16 finished\n",
      "Cost on training data: 0.3831580571924589\n",
      "Accuracy on training data: 48232 / 50000\n",
      "Cost on evaluation data: 0.9479745320717807\n",
      "Accuracy on evaluation data: 9594 / 10000\n",
      "Epoch 17 finished\n",
      "Cost on training data: 0.40989873931983306\n",
      "Accuracy on training data: 48036 / 50000\n",
      "Cost on evaluation data: 0.9757574843066421\n",
      "Accuracy on evaluation data: 9540 / 10000\n",
      "Epoch 18 finished\n",
      "Cost on training data: 0.3961838970764535\n",
      "Accuracy on training data: 48120 / 50000\n",
      "Cost on evaluation data: 0.9726472133398734\n",
      "Accuracy on evaluation data: 9544 / 10000\n",
      "Epoch 19 finished\n",
      "Cost on training data: 0.36719976300537027\n",
      "Accuracy on training data: 48430 / 50000\n",
      "Cost on evaluation data: 0.9412198988366378\n",
      "Accuracy on evaluation data: 9604 / 10000\n",
      "Epoch 20 finished\n",
      "Cost on training data: 0.4047447570809608\n",
      "Accuracy on training data: 48180 / 50000\n",
      "Cost on evaluation data: 0.9754419391969944\n",
      "Accuracy on evaluation data: 9565 / 10000\n",
      "Epoch 21 finished\n",
      "Cost on training data: 0.36231011970711857\n",
      "Accuracy on training data: 48469 / 50000\n",
      "Cost on evaluation data: 0.9401698918079984\n",
      "Accuracy on evaluation data: 9603 / 10000\n",
      "Epoch 22 finished\n",
      "Cost on training data: 0.36562993811459465\n",
      "Accuracy on training data: 48522 / 50000\n",
      "Cost on evaluation data: 0.9427986859370483\n",
      "Accuracy on evaluation data: 9629 / 10000\n",
      "Epoch 23 finished\n",
      "Cost on training data: 0.3587845803631595\n",
      "Accuracy on training data: 48542 / 50000\n",
      "Cost on evaluation data: 0.9269646148987378\n",
      "Accuracy on evaluation data: 9646 / 10000\n",
      "Epoch 24 finished\n",
      "Cost on training data: 0.38134551670822897\n",
      "Accuracy on training data: 48368 / 50000\n",
      "Cost on evaluation data: 0.9542678212924507\n",
      "Accuracy on evaluation data: 9614 / 10000\n",
      "Epoch 25 finished\n",
      "Cost on training data: 0.38296041270319836\n",
      "Accuracy on training data: 48320 / 50000\n",
      "Cost on evaluation data: 0.9517752643698862\n",
      "Accuracy on evaluation data: 9592 / 10000\n",
      "Epoch 26 finished\n",
      "Cost on training data: 0.3756614483073528\n",
      "Accuracy on training data: 48390 / 50000\n",
      "Cost on evaluation data: 0.9589403480139034\n",
      "Accuracy on evaluation data: 9592 / 10000\n",
      "Epoch 27 finished\n",
      "Cost on training data: 0.3716125851028263\n",
      "Accuracy on training data: 48489 / 50000\n",
      "Cost on evaluation data: 0.9446278746980826\n",
      "Accuracy on evaluation data: 9608 / 10000\n",
      "Epoch 28 finished\n",
      "Cost on training data: 0.38799249646068756\n",
      "Accuracy on training data: 48315 / 50000\n",
      "Cost on evaluation data: 0.9598055417475202\n",
      "Accuracy on evaluation data: 9598 / 10000\n",
      "Epoch 29 finished\n",
      "Cost on training data: 0.37494414893162764\n",
      "Accuracy on training data: 48435 / 50000\n",
      "Cost on evaluation data: 0.957016171823357\n",
      "Accuracy on evaluation data: 9602 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([0.8060758939928532,\n",
       "  0.8541352821096009,\n",
       "  0.8902788838316347,\n",
       "  0.8958818274455034,\n",
       "  0.9080913552600751,\n",
       "  0.9146304304273882,\n",
       "  0.9357288455719278,\n",
       "  0.9216804361350406,\n",
       "  0.9354999911181375,\n",
       "  0.9544562972892887,\n",
       "  0.9434817734097319,\n",
       "  0.9273468769595854,\n",
       "  0.9337514447400954,\n",
       "  0.9932563665210454,\n",
       "  0.9673846439002207,\n",
       "  0.9398237025245462,\n",
       "  0.9479745320717807,\n",
       "  0.9757574843066421,\n",
       "  0.9726472133398734,\n",
       "  0.9412198988366378,\n",
       "  0.9754419391969944,\n",
       "  0.9401698918079984,\n",
       "  0.9427986859370483,\n",
       "  0.9269646148987378,\n",
       "  0.9542678212924507,\n",
       "  0.9517752643698862,\n",
       "  0.9589403480139034,\n",
       "  0.9446278746980826,\n",
       "  0.9598055417475202,\n",
       "  0.957016171823357],\n",
       " [9404,\n",
       "  9494,\n",
       "  9527,\n",
       "  9561,\n",
       "  9567,\n",
       "  9574,\n",
       "  9567,\n",
       "  9590,\n",
       "  9578,\n",
       "  9571,\n",
       "  9597,\n",
       "  9620,\n",
       "  9609,\n",
       "  9506,\n",
       "  9564,\n",
       "  9604,\n",
       "  9594,\n",
       "  9540,\n",
       "  9544,\n",
       "  9604,\n",
       "  9565,\n",
       "  9603,\n",
       "  9629,\n",
       "  9646,\n",
       "  9614,\n",
       "  9592,\n",
       "  9592,\n",
       "  9608,\n",
       "  9598,\n",
       "  9602],\n",
       " [0.49861334110702016,\n",
       "  0.44704183452272317,\n",
       "  0.4222718170221894,\n",
       "  0.40518845628272937,\n",
       "  0.3959622088613118,\n",
       "  0.3857796520157049,\n",
       "  0.39863137155241113,\n",
       "  0.3860621541637011,\n",
       "  0.3898062600109953,\n",
       "  0.3961811046382846,\n",
       "  0.3929240474516481,\n",
       "  0.3685762153672637,\n",
       "  0.37326766728163746,\n",
       "  0.41901052658864035,\n",
       "  0.3973309381719917,\n",
       "  0.3746451906630837,\n",
       "  0.3831580571924589,\n",
       "  0.40989873931983306,\n",
       "  0.3961838970764535,\n",
       "  0.36719976300537027,\n",
       "  0.4047447570809608,\n",
       "  0.36231011970711857,\n",
       "  0.36562993811459465,\n",
       "  0.3587845803631595,\n",
       "  0.38134551670822897,\n",
       "  0.38296041270319836,\n",
       "  0.3756614483073528,\n",
       "  0.3716125851028263,\n",
       "  0.38799249646068756,\n",
       "  0.37494414893162764],\n",
       " [46918,\n",
       "  47613,\n",
       "  47898,\n",
       "  47967,\n",
       "  48121,\n",
       "  48151,\n",
       "  48038,\n",
       "  48287,\n",
       "  48250,\n",
       "  48077,\n",
       "  48255,\n",
       "  48408,\n",
       "  48355,\n",
       "  47935,\n",
       "  48255,\n",
       "  48403,\n",
       "  48232,\n",
       "  48036,\n",
       "  48120,\n",
       "  48430,\n",
       "  48180,\n",
       "  48469,\n",
       "  48522,\n",
       "  48542,\n",
       "  48368,\n",
       "  48320,\n",
       "  48390,\n",
       "  48489,\n",
       "  48315,\n",
       "  48435])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 0.5,lmbda = 5.0,evaluation_data=validation_data,\n",
    " monitor_evaluation_accuracy=True,\n",
    " monitor_evaluation_cost=True,\n",
    " monitor_training_accuracy=True,\n",
    " monitor_training_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = Network([784, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 1 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 2 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 3 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 4 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 5 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 6 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 7 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 8 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 9 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 10 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 11 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 12 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 13 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 14 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 15 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 16 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 17 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 18 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 19 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 20 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 21 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 22 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 23 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 24 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 25 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 26 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 27 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 28 finished\n",
      "Accuracy on evaluation data: 0 / 0\n",
      "Epoch 29 finished\n",
      "Accuracy on evaluation data: 0 / 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.SGD(training_data, 30, 10, 0.1, lmbda=5.0,evaluation_data=validation_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "net2 = Network([784, 30, 30, 10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 finished\n",
      "Accuracy on evaluation data: 9176 / 10000\n",
      "Epoch 1 finished\n",
      "Accuracy on evaluation data: 9431 / 10000\n",
      "Epoch 2 finished\n",
      "Accuracy on evaluation data: 9522 / 10000\n",
      "Epoch 3 finished\n",
      "Accuracy on evaluation data: 9567 / 10000\n",
      "Epoch 4 finished\n",
      "Accuracy on evaluation data: 9571 / 10000\n",
      "Epoch 5 finished\n",
      "Accuracy on evaluation data: 9626 / 10000\n",
      "Epoch 6 finished\n",
      "Accuracy on evaluation data: 9635 / 10000\n",
      "Epoch 7 finished\n",
      "Accuracy on evaluation data: 9625 / 10000\n",
      "Epoch 8 finished\n",
      "Accuracy on evaluation data: 9662 / 10000\n",
      "Epoch 9 finished\n",
      "Accuracy on evaluation data: 9655 / 10000\n",
      "Epoch 10 finished\n",
      "Accuracy on evaluation data: 9657 / 10000\n",
      "Epoch 11 finished\n",
      "Accuracy on evaluation data: 9680 / 10000\n",
      "Epoch 12 finished\n",
      "Accuracy on evaluation data: 9689 / 10000\n",
      "Epoch 13 finished\n",
      "Accuracy on evaluation data: 9683 / 10000\n",
      "Epoch 14 finished\n",
      "Accuracy on evaluation data: 9698 / 10000\n",
      "Epoch 15 finished\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "Epoch 16 finished\n",
      "Accuracy on evaluation data: 9691 / 10000\n",
      "Epoch 17 finished\n",
      "Accuracy on evaluation data: 9704 / 10000\n",
      "Epoch 18 finished\n",
      "Accuracy on evaluation data: 9704 / 10000\n",
      "Epoch 19 finished\n",
      "Accuracy on evaluation data: 9697 / 10000\n",
      "Epoch 20 finished\n",
      "Accuracy on evaluation data: 9693 / 10000\n",
      "Epoch 21 finished\n",
      "Accuracy on evaluation data: 9697 / 10000\n",
      "Epoch 22 finished\n",
      "Accuracy on evaluation data: 9696 / 10000\n",
      "Epoch 23 finished\n",
      "Accuracy on evaluation data: 9689 / 10000\n",
      "Epoch 24 finished\n",
      "Accuracy on evaluation data: 9710 / 10000\n",
      "Epoch 25 finished\n",
      "Accuracy on evaluation data: 9688 / 10000\n",
      "Epoch 26 finished\n",
      "Accuracy on evaluation data: 9702 / 10000\n",
      "Epoch 27 finished\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "Epoch 28 finished\n",
      "Accuracy on evaluation data: 9703 / 10000\n",
      "Epoch 29 finished\n",
      "Accuracy on evaluation data: 9710 / 10000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([],\n",
       " [9176,\n",
       "  9431,\n",
       "  9522,\n",
       "  9567,\n",
       "  9571,\n",
       "  9626,\n",
       "  9635,\n",
       "  9625,\n",
       "  9662,\n",
       "  9655,\n",
       "  9657,\n",
       "  9680,\n",
       "  9689,\n",
       "  9683,\n",
       "  9698,\n",
       "  9703,\n",
       "  9691,\n",
       "  9704,\n",
       "  9704,\n",
       "  9697,\n",
       "  9693,\n",
       "  9697,\n",
       "  9696,\n",
       "  9689,\n",
       "  9710,\n",
       "  9688,\n",
       "  9702,\n",
       "  9703,\n",
       "  9703,\n",
       "  9710],\n",
       " [],\n",
       " [])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net2.SGD(training_data, 30, 10, 0.1, lmbda=5.0,evaluation_data=validation_data, monitor_evaluation_accuracy=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
